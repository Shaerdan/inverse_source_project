\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}}
\newcommand{\x}{\bm{x}}
\newcommand{\y}{\bm{y}}
\newcommand{\n}{\bm{n}}
\newcommand{\q}{\bm{q}}
\newcommand{\grad}{\nabla}
\newcommand{\dive}{\nabla \cdot}
\newcommand{\lapl}{\Delta}
\newcommand{\norm}[1]{\left\| #1 \right\|}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\argmin}{\operatorname*{argmin}}

\title{Inverse Source Localization: Complete Mathematical Formulation\\[0.5em]
\large Part 3: Inverse Problem Formulations}
\author{Mathematical Reference Document\\
Version 7.23}
\date{January 2026}

\begin{document}
\maketitle

\begin{abstract}
This document presents the two main approaches to the inverse source localization problem: the linear formulation (distributed sources on a fixed grid) with L1, L2, and Total Variation regularization, and the nonlinear formulation (continuous point source positions). Complete derivations of optimality conditions and solution algorithms are provided.
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{The Inverse Problem}
%==============================================================================

\subsection{Problem Statement}

\textbf{Given:} Noisy boundary measurements $\tilde{\bm{u}} \in \R^{N_b}$:
\begin{equation}
\tilde{\bm{u}} = \bm{u}^{\text{true}} + \bm{\eta}, \quad \bm{\eta} \sim \mathcal{N}(\bm{0}, \sigma^2 \bm{I})
\end{equation}

\textbf{Find:} Source locations $\{\bm{z}_k\}$ and intensities $\{q_k\}$ that generated $\bm{u}^{\text{true}}$.

\subsection{Two Formulations}

\begin{enumerate}
\item \textbf{Linear (Distributed Sources):} Fix candidate source locations on a grid. Solve for intensities only.
\begin{itemize}
\item Unknowns: $\q \in \R^M$ (intensities at $M$ grid points)
\item Problem: Convex optimization
\item Sources discovered via sparsity-promoting regularization
\end{itemize}

\item \textbf{Nonlinear (Point Sources):} Solve for both positions and intensities.
\begin{itemize}
\item Unknowns: $\bm{\theta} = (x_1, y_1, q_1, \ldots, x_K, y_K, q_K) \in \R^{3K}$
\item Problem: Non-convex optimization
\item Number of sources $K$ must be specified
\end{itemize}
\end{enumerate}

%==============================================================================
\section{Linear Inverse Problem}
%==============================================================================

\subsection{Setup}

\subsubsection{Source Grid}

Fix $M$ candidate source locations $\{\bm{\xi}_j\}_{j=1}^M$ inside the domain $\Omega$.

\subsubsection{Green's Matrix}

Define $\bm{G} \in \R^{N_b \times M}$ with:
\begin{equation}
G_{ij} = G_N(\x_i^{\text{boundary}}, \bm{\xi}_j)
\end{equation}

The forward model is:
\begin{equation}
\bm{u}_{\text{boundary}} = \bm{G} \q
\label{eq:forward_linear}
\end{equation}

\subsubsection{Compatibility Constraint}

From the physics: $\bm{1}^T \q = \sum_{j=1}^M q_j = 0$.

\subsubsection{Ill-Posedness}

The linear system $\bm{G}\q = \tilde{\bm{u}}$ is typically:
\begin{itemize}
\item Underdetermined ($M > N_b$): infinitely many solutions
\item Ill-conditioned: small noise causes large errors in naive inversion
\end{itemize}

Regularization is essential.

\subsection{L2 Regularization (Tikhonov)}

\subsubsection{Optimization Problem}

\begin{equation}
\boxed{\min_{\q \in \R^M} \frac{1}{2}\norm{\bm{G}\q - \tilde{\bm{u}}}_2^2 + \frac{\alpha}{2}\norm{\q}_2^2 \quad \text{s.t.} \quad \bm{1}^T\q = 0}
\label{eq:l2_problem}
\end{equation}

where $\alpha > 0$ is the regularization parameter.

\subsubsection{Lagrangian}

\begin{equation}
\mathcal{L}(\q, \lambda) = \frac{1}{2}\norm{\bm{G}\q - \tilde{\bm{u}}}_2^2 + \frac{\alpha}{2}\norm{\q}_2^2 + \lambda \bm{1}^T\q
\end{equation}

\subsubsection{Optimality Conditions (KKT)}

\textbf{Stationarity:}
\begin{align}
\nabla_\q \mathcal{L} &= \bm{G}^T(\bm{G}\q - \tilde{\bm{u}}) + \alpha\q + \lambda\bm{1} = \bm{0} \label{eq:l2_stationarity}
\end{align}

\textbf{Primal feasibility:}
\begin{equation}
\bm{1}^T\q = 0 \label{eq:l2_feasibility}
\end{equation}

\subsubsection{Solution Derivation}

\textbf{Step 1: Rearrange stationarity condition.}

From \eqref{eq:l2_stationarity}:
\begin{equation}
(\bm{G}^T\bm{G} + \alpha\bm{I})\q = \bm{G}^T\tilde{\bm{u}} - \lambda\bm{1}
\end{equation}

Define $\bm{A} = \bm{G}^T\bm{G} + \alpha\bm{I}$. Note: $\bm{A}$ is symmetric positive definite (SPD) for $\alpha > 0$.

\begin{equation}
\q = \bm{A}^{-1}(\bm{G}^T\tilde{\bm{u}} - \lambda\bm{1})
\label{eq:l2_q_lambda}
\end{equation}

\textbf{Step 2: Apply feasibility constraint.}

Substitute \eqref{eq:l2_q_lambda} into \eqref{eq:l2_feasibility}:
\begin{align}
0 &= \bm{1}^T\q \\
&= \bm{1}^T\bm{A}^{-1}(\bm{G}^T\tilde{\bm{u}} - \lambda\bm{1}) \\
&= \bm{1}^T\bm{A}^{-1}\bm{G}^T\tilde{\bm{u}} - \lambda\bm{1}^T\bm{A}^{-1}\bm{1}
\end{align}

Solving for $\lambda$:
\begin{equation}
\boxed{\lambda^* = \frac{\bm{1}^T\bm{A}^{-1}\bm{G}^T\tilde{\bm{u}}}{\bm{1}^T\bm{A}^{-1}\bm{1}}}
\label{eq:l2_lambda}
\end{equation}

\textbf{Step 3: Final solution.}

\begin{equation}
\boxed{\q^* = \bm{A}^{-1}(\bm{G}^T\tilde{\bm{u}} - \lambda^*\bm{1})}
\label{eq:l2_solution}
\end{equation}

\subsubsection{Algorithm}

\begin{algorithm}
\caption{L2 Regularized Solution}
\begin{algorithmic}[1]
\Require Green's matrix $\bm{G}$, measurements $\tilde{\bm{u}}$, regularization $\alpha$
\State $\bm{A} \gets \bm{G}^T\bm{G} + \alpha\bm{I}$
\State Solve $\bm{A}\bm{v} = \bm{G}^T\tilde{\bm{u}}$ for $\bm{v}$
\State Solve $\bm{A}\bm{w} = \bm{1}$ for $\bm{w}$
\State $\lambda^* \gets (\bm{1}^T\bm{v}) / (\bm{1}^T\bm{w})$
\State $\q^* \gets \bm{v} - \lambda^*\bm{w}$
\State \Return $\q^*$
\end{algorithmic}
\end{algorithm}

\subsection{L1 Regularization (Lasso / Sparsity-Promoting)}

\subsubsection{Optimization Problem}

\begin{equation}
\boxed{\min_{\q \in \R^M} \frac{1}{2}\norm{\bm{G}\q - \tilde{\bm{u}}}_2^2 + \alpha\norm{\q}_1 \quad \text{s.t.} \quad \bm{1}^T\q = 0}
\label{eq:l1_problem}
\end{equation}

where $\norm{\q}_1 = \sum_j \abs{q_j}$.

\subsubsection{Why L1 Promotes Sparsity}

\begin{theorem}[Sparsity of L1 Solutions]
The L1 regularizer promotes sparsity: optimal solutions tend to have many zero components.
\end{theorem}

\begin{proof}[Geometric Intuition]
The L1 unit ball $\{\q : \norm{\q}_1 \leq 1\}$ has corners at $\pm\bm{e}_j$ (standard basis vectors). The level sets of the objective (ellipsoids for quadratic loss) are more likely to touch the L1 ball at corners, yielding sparse solutions.
\end{proof}

\subsubsection{Optimality Conditions}

The L1 norm is non-differentiable. We use the subdifferential:
\begin{equation}
\partial\norm{\q}_1 = \{g \in \R^M : g_j \in \partial\abs{q_j}\}
\end{equation}
where:
\begin{equation}
\partial\abs{q_j} = \begin{cases}
\{+1\} & q_j > 0 \\
\{-1\} & q_j < 0 \\
[-1, +1] & q_j = 0
\end{cases}
\end{equation}

\textbf{KKT conditions:}
\begin{align}
\bm{0} &\in \bm{G}^T(\bm{G}\q - \tilde{\bm{u}}) + \alpha\,\partial\norm{\q}_1 + \lambda\bm{1} \\
0 &= \bm{1}^T\q
\end{align}

Component-wise: for each $j$,
\begin{equation}
\begin{cases}
(\bm{G}^T(\bm{G}\q - \tilde{\bm{u}}))_j + \alpha\,\text{sign}(q_j) + \lambda = 0 & \text{if } q_j \neq 0 \\
\abs{(\bm{G}^T(\bm{G}\q - \tilde{\bm{u}}))_j + \lambda} \leq \alpha & \text{if } q_j = 0
\end{cases}
\end{equation}

\subsubsection{Solution via CVXPY}

In practice, we use convex optimization solvers. The problem \eqref{eq:l1_problem} can be reformulated as a quadratic program:

\begin{align}
\min_{\q, \bm{t}} \quad & \frac{1}{2}\norm{\bm{G}\q - \tilde{\bm{u}}}_2^2 + \alpha \bm{1}^T\bm{t} \\
\text{s.t.} \quad & -\bm{t} \leq \q \leq \bm{t} \\
& \bm{1}^T\q = 0
\end{align}

where $\bm{t} \geq \bm{0}$ enforces $t_j \geq \abs{q_j}$ at optimum.

\subsection{Total Variation (TV) Regularization}

\subsubsection{Motivation}

TV regularization promotes \textit{piecewise constant} solutions, penalizing spatial variation rather than magnitude.

\subsubsection{Discrete Gradient Operator}

\begin{definition}[Gradient Operator]
On a 2D grid or mesh, define $\bm{D} \in \R^{E \times M}$ where $E$ is the number of edges:
\begin{equation}
(\bm{D}\q)_e = q_j - q_i
\end{equation}
for edge $e$ connecting nodes $i$ and $j$.
\end{definition}

For a Delaunay mesh, we use edges from the triangulation. For a regular grid:
\begin{align}
\bm{D} = \begin{bmatrix} \bm{D}_x \\ \bm{D}_y \end{bmatrix}
\end{align}
where $\bm{D}_x$ and $\bm{D}_y$ are first-difference operators in $x$ and $y$.

\subsubsection{Anisotropic TV}

\begin{equation}
\text{TV}(\q) = \norm{\bm{D}\q}_1 = \sum_e \abs{(\bm{D}\q)_e}
\end{equation}

\subsubsection{Optimization Problem}

\begin{equation}
\boxed{\min_{\q \in \R^M} \frac{1}{2}\norm{\bm{G}\q - \tilde{\bm{u}}}_2^2 + \alpha\norm{\bm{D}\q}_1 \quad \text{s.t.} \quad \bm{1}^T\q = 0}
\label{eq:tv_problem}
\end{equation}

\subsubsection{Solution via ADMM}

The Alternating Direction Method of Multipliers (ADMM) is effective for TV problems.

\textbf{Variable splitting:} Introduce $\bm{z} = \bm{D}\q$:
\begin{align}
\min_{\q, \bm{z}} \quad & \frac{1}{2}\norm{\bm{G}\q - \tilde{\bm{u}}}_2^2 + \alpha\norm{\bm{z}}_1 \\
\text{s.t.} \quad & \bm{D}\q = \bm{z}, \quad \bm{1}^T\q = 0
\end{align}

\textbf{Augmented Lagrangian:}
\begin{equation}
L_\rho(\q, \bm{z}, \bm{y}) = \frac{1}{2}\norm{\bm{G}\q - \tilde{\bm{u}}}_2^2 + \alpha\norm{\bm{z}}_1 + \bm{y}^T(\bm{D}\q - \bm{z}) + \frac{\rho}{2}\norm{\bm{D}\q - \bm{z}}_2^2
\end{equation}

\textbf{ADMM iterations:}
\begin{align}
\q^{k+1} &= \argmin_{\q: \bm{1}^T\q=0} \frac{1}{2}\norm{\bm{G}\q - \tilde{\bm{u}}}_2^2 + \frac{\rho}{2}\norm{\bm{D}\q - \bm{z}^k + \bm{u}^k}_2^2 \\
\bm{z}^{k+1} &= \argmin_{\bm{z}} \alpha\norm{\bm{z}}_1 + \frac{\rho}{2}\norm{\bm{D}\q^{k+1} - \bm{z} + \bm{u}^k}_2^2 \\
\bm{u}^{k+1} &= \bm{u}^k + \bm{D}\q^{k+1} - \bm{z}^{k+1}
\end{align}

where $\bm{u} = \bm{y}/\rho$ (scaled dual variable).

\textbf{$\q$-subproblem:} Linear system with constraint:
\begin{equation}
(\bm{G}^T\bm{G} + \rho\bm{D}^T\bm{D})\q = \bm{G}^T\tilde{\bm{u}} + \rho\bm{D}^T(\bm{z}^k - \bm{u}^k) - \lambda\bm{1}
\end{equation}
where $\lambda$ enforces $\bm{1}^T\q = 0$.

\textbf{$\bm{z}$-subproblem:} Soft thresholding:
\begin{equation}
z_e^{k+1} = S_{\alpha/\rho}((\bm{D}\q^{k+1})_e + u_e^k)
\end{equation}
where $S_\tau(x) = \text{sign}(x)\max(\abs{x} - \tau, 0)$ is the soft threshold operator.

\subsection{Parameter Selection: L-Curve Method}

\subsubsection{The L-Curve}

For each $\alpha$, compute the solution $\q(\alpha)$ and plot:
\begin{itemize}
\item $x$-axis: $\log_{10}\norm{\bm{G}\q(\alpha) - \tilde{\bm{u}}}_2$ (residual)
\item $y$-axis: $\log_{10}R(\q(\alpha))$ (regularizer: $\norm{\q}_2$, $\norm{\q}_1$, or $\norm{\bm{D}\q}_1$)
\end{itemize}

The curve typically has an "L" shape:
\begin{itemize}
\item \textbf{Large $\alpha$:} Over-regularized, smooth but poor fit
\item \textbf{Small $\alpha$:} Under-regularized, good fit but noisy
\item \textbf{Corner:} Optimal trade-off
\end{itemize}

\subsubsection{Corner Detection}

\begin{definition}[L-Curve Corner]
The optimal $\alpha^*$ is at the point of maximum curvature:
\begin{equation}
\alpha^* = \argmax_\alpha \kappa(\alpha)
\end{equation}
where $\kappa$ is the curvature of the L-curve.
\end{definition}

In log-log space with $x = \log r$, $y = \log\rho$:
\begin{equation}
\kappa = \frac{x'y'' - x''y'}{(x'^2 + y'^2)^{3/2}}
\end{equation}

\textbf{Practical implementation:} Use discrete approximation or the ``maximum distance from line'' heuristic:
\begin{enumerate}
\item Draw line from first to last point on L-curve
\item Find point with maximum perpendicular distance from line
\end{enumerate}

%==============================================================================
\section{Nonlinear Inverse Problem}
%==============================================================================

\subsection{Problem Formulation}

\textbf{Unknowns:} For $K$ sources, $\bm{\theta} = (x_1, y_1, q_1, \ldots, x_K, y_K, q_K) \in \R^{3K}$.

\textbf{Objective:}
\begin{equation}
\boxed{J(\bm{\theta}) = \frac{1}{2}\norm{\bm{u}^{\text{forward}}(\bm{\theta}) - \tilde{\bm{u}}}_2^2}
\label{eq:nonlinear_objective}
\end{equation}

where $\bm{u}^{\text{forward}}(\bm{\theta})$ computes boundary values from sources at $(x_k, y_k)$ with intensities $q_k$.

\textbf{Constraints:}
\begin{align}
(x_k, y_k) &\in \Omega \quad \text{(sources inside domain)} \\
\sum_{k=1}^K q_k &= 0 \quad \text{(compatibility)}
\end{align}

\subsection{Why It's Nonlinear}

The forward model is:
\begin{equation}
u_i^{\text{forward}} = \sum_{k=1}^K q_k \, G_N(\x_i, (x_k, y_k))
\end{equation}

This is \textbf{linear} in $q_k$ but \textbf{nonlinear} in $(x_k, y_k)$ because $G_N(\x, \bm{\xi})$ depends nonlinearly on $\bm{\xi}$.

\subsection{Gradient Computation}

For gradient-based optimization, we need:
\begin{equation}
\nabla_{\bm{\theta}} J = \bm{J}^T (\bm{u}^{\text{forward}} - \tilde{\bm{u}})
\end{equation}
where $\bm{J} = \frac{\partial \bm{u}^{\text{forward}}}{\partial \bm{\theta}}$ is the Jacobian.

\subsubsection{Jacobian Components}

For the $k$-th source:
\begin{align}
\frac{\partial u_i}{\partial x_k} &= q_k \frac{\partial G_N}{\partial \xi_1}(\x_i, (x_k, y_k)) \\
\frac{\partial u_i}{\partial y_k} &= q_k \frac{\partial G_N}{\partial \xi_2}(\x_i, (x_k, y_k)) \\
\frac{\partial u_i}{\partial q_k} &= G_N(\x_i, (x_k, y_k))
\end{align}

\subsubsection{Green's Function Gradient (Unit Disk)}

From Part 1, using complex notation:
\begin{equation}
G_N(z, \zeta) = -\frac{1}{2\pi}\ln\abs{z - \zeta} - \frac{1}{2\pi}\ln\abs{1 - z\bar{\zeta}} + C
\end{equation}

The gradient with respect to source position $\zeta = (\xi_1, \xi_2)$:
\begin{align}
\frac{\partial G_N}{\partial \xi_1} &= -\frac{1}{2\pi}\left(\frac{-(x - \xi_1)}{\abs{\x - \bm{\xi}}^2} + \frac{x}{\abs{1 - z\bar{\zeta}}^2}\right) \\
\frac{\partial G_N}{\partial \xi_2} &= -\frac{1}{2\pi}\left(\frac{-(y - \xi_2)}{\abs{\x - \bm{\xi}}^2} + \frac{y}{\abs{1 - z\bar{\zeta}}^2}\right)
\end{align}

\subsection{Optimization Methods}

\subsubsection{L-BFGS-B}

Limited-memory BFGS with box constraints.

\textbf{Advantages:}
\begin{itemize}
\item Fast convergence for smooth problems
\item Handles box constraints (bounds on positions)
\item Moderate memory usage
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
\item Finds local minima only
\item Sensitive to initialization
\end{itemize}

\subsubsection{Differential Evolution}

Global optimization via evolutionary algorithm.

\textbf{Advantages:}
\begin{itemize}
\item Global search (escapes local minima)
\item No gradient required
\item Robust to noise
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
\item Slow (many function evaluations)
\item May not converge to exact optimum
\end{itemize}

\subsubsection{Multi-Start Strategy}

Run L-BFGS-B from multiple random initializations:
\begin{enumerate}
\item Sample $N_{\text{start}}$ initial configurations
\item Run L-BFGS-B from each
\item Return best solution
\end{enumerate}

\subsection{Handling the Compatibility Constraint}

\textbf{Method 1: Elimination.}
Set $q_K = -\sum_{k=1}^{K-1} q_k$, reducing unknowns to $3K - 1$.

\textbf{Method 2: Penalty.}
Add penalty term $\mu(\sum_k q_k)^2$ to objective.

\textbf{Method 3: Projection.}
After each optimization step, project: $q_k \gets q_k - \bar{q}$ where $\bar{q} = \frac{1}{K}\sum_k q_k$.

%==============================================================================
\section{Quality Metrics}
%==============================================================================

\subsection{For Linear Solvers}

Traditional metrics (RMSE of peaks) are \textbf{misleading} for distributed solutions.

\subsubsection{Localization Score}

\begin{definition}[Localization Score]
\begin{equation}
S_{\text{loc}} = \frac{\sum_{j=1}^M \abs{q_j} \cdot w_j}{\sum_{j=1}^M \abs{q_j}}
\end{equation}
where $w_j = \max_k \exp\left(-\frac{\norm{\bm{\xi}_j - \bm{z}_k^{\text{true}}}^2}{2\sigma^2}\right)$ is the Gaussian weight to nearest true source.
\end{definition}

\textbf{Interpretation:}
\begin{itemize}
\item $S_{\text{loc}} = 1$: All intensity concentrated exactly at true sources
\item $S_{\text{loc}} \approx 0$: Intensity far from true sources
\end{itemize}

\subsubsection{Sparsity Ratio}

\begin{definition}[Sparsity Ratio]
\begin{equation}
S_{\text{spar}} = \min\left(\frac{K_{\text{target}}}{N_{90\%}}, 1\right)
\end{equation}
where $N_{90\%}$ is the number of points containing 90\% of total intensity.
\end{definition}

\textbf{Interpretation:}
\begin{itemize}
\item $S_{\text{spar}} = 1$: Intensity in exactly $K_{\text{target}}$ points (sparse)
\item $S_{\text{spar}} \approx 0$: Intensity spread across many points (diffuse)
\end{itemize}

\subsection{For Nonlinear Solvers}

\subsubsection{Position RMSE}

\begin{equation}
\text{RMSE}_{\text{pos}} = \sqrt{\frac{1}{K}\sum_{k=1}^K \norm{\bm{z}_k^{\text{rec}} - \bm{z}_{\pi(k)}^{\text{true}}}^2}
\end{equation}
where $\pi$ is the optimal matching between recovered and true sources (Hungarian algorithm).

\subsubsection{Intensity RMSE}

\begin{equation}
\text{RMSE}_{\text{int}} = \sqrt{\frac{1}{K}\sum_{k=1}^K (q_k^{\text{rec}} - q_{\pi(k)}^{\text{true}})^2}
\end{equation}

\subsubsection{Boundary Residual}

\begin{equation}
\text{Res} = \frac{\norm{\bm{u}^{\text{forward}}(\bm{\theta}^{\text{rec}}) - \tilde{\bm{u}}}_2}{\norm{\tilde{\bm{u}}}_2}
\end{equation}

%==============================================================================
\section{Summary of Algorithms}
%==============================================================================

\subsection{Linear Inverse (Distributed)}

\begin{enumerate}
\item \textbf{Build Green's matrix} $\bm{G}$ (forward solves for each grid point)
\item \textbf{Select $\alpha$} via L-curve
\item \textbf{Solve regularized problem}:
\begin{itemize}
\item L1: CVXPY or coordinate descent
\item L2: Closed-form (Equations \ref{eq:l2_lambda}--\ref{eq:l2_solution})
\item TV: ADMM
\end{itemize}
\item \textbf{Evaluate} localization score, sparsity ratio
\end{enumerate}

\subsection{Nonlinear Inverse (Point Sources)}

\begin{enumerate}
\item \textbf{Specify} number of sources $K$
\item \textbf{Initialize} positions (random or from linear solution peaks)
\item \textbf{Optimize}:
\begin{itemize}
\item L-BFGS-B with multi-start, or
\item Differential evolution
\end{itemize}
\item \textbf{Enforce} compatibility $\sum q_k = 0$
\item \textbf{Evaluate} position RMSE, intensity RMSE
\end{enumerate}

\vspace{1em}
\hrule
\vspace{1em}
\textit{End of Mathematical Formulation}

\appendix

\section{Notation Summary}

\begin{tabular}{ll}
\hline
Symbol & Meaning \\
\hline
$\Omega$ & Domain (open, bounded, simply connected) \\
$\partial\Omega$ & Boundary of domain \\
$\D$ & Unit disk $\{z : \abs{z} < 1\}$ \\
$\lapl$ & Laplacian operator \\
$\grad$ & Gradient operator \\
$G_N$ & Neumann Green's function \\
$G_0$ & Free-space Green's function \\
$\bm{z}_k$ & True source position \\
$q_k$ & Source intensity \\
$\bm{\xi}_j$ & Grid point (candidate source location) \\
$\bm{G}$ & Green's matrix ($N_b \times M$) \\
$\q$ & Source intensity vector \\
$\tilde{\bm{u}}$ & Measured boundary values \\
$\alpha$ & Regularization parameter \\
$\bm{D}$ & Discrete gradient operator \\
\hline
\end{tabular}

\end{document}
